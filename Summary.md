# Important Links for reference
- [Solid Principles](https://towardsdatascience.com/data-science-from-school-to-work-part-ii/#:~:text=UV%20tool.-,The%20SOLID%20principles,-SOLID%20programming%20is)
- [Advice by a fellow ML Engineer](https://towardsdatascience.com/my-honest-advice-for-aspiring-machine-learning-engineers/)
- Types of Activation functions

### Types of Activation function
  1. Linear Activation function - g(z) = z which is xD "no activation function"
  2. Sigmoid Activation function - g(z) = 1/(1+e**-z)
  3. ReLU - Rectified Linear Unit - g(z) = max(0, z)
